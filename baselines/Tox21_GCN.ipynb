{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Tox21_GCN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihal-rao/deepchem/blob/master/baselines/Tox21_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW-q4kpKnwm6"
      },
      "source": [
        "##Installing DeepChem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyo3N42_np0L"
      },
      "source": [
        "DeepChem is a python-based open source deep learning framework and offers feature rich set toolchain that democratizes the use of deep-learning in drug discovery, materials science, quantum chemistry, and biology."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glV7StXjjzt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bbc620-008e-4404-8799-1e73524be839"
      },
      "source": [
        "!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import conda_installer\n",
        "conda_installer.install()\n",
        "!/root/miniconda/bin/conda info -e"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3501  100  3501    0     0  16671      0 --:--:-- --:--:-- --:--:-- 16671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.7/site-packages to PYTHONPATH\n",
            "python version: 3.7.10\n",
            "fetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "done\n",
            "installing miniconda to /root/miniconda\n",
            "done\n",
            "installing rdkit, openmm, pdbfixer\n",
            "added conda-forge to channels\n",
            "added omnia to channels\n",
            "done\n",
            "conda packages installation finished!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /root/miniconda\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo9dHE2sjzuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd030161-6c4f-457e-9ca2-c2e7cd9096f4"
      },
      "source": [
        "!pip install --pre deepchem\n",
        "!pip install dgl\n",
        "!pip install dgllife"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepchem\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b7/b2f36388bdd60420d2f6923076a30c57ca08557b7fa6b63e720440188c13/deepchem-2.6.0.dev20210323214627-py3-none-any.whl (552kB)\n",
            "\r\u001b[K     |▋                               | 10kB 11.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 92kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 419kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 430kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 440kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 450kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 460kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 471kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 481kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 491kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 501kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 512kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 522kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 532kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 542kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 552kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deepchem) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from deepchem) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->deepchem) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Installing collected packages: deepchem\n",
            "Successfully installed deepchem-2.6.0.dev20210323214627\n",
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/72/61668fa3ef059f889f98653197e6f93592735660298042ffc5adb2005ca5/dgl-0.6.0.post1-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.0.post1\n",
            "Collecting dgllife\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/f3/04509470ce7fad6d647c8b41ffff8984b7691260eb78b305997104b09ebc/dgllife-0.2.6.tar.gz (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from dgllife) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgllife) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgllife) (2.5)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from dgllife) (0.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->dgllife) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->dgllife) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (2020.12.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgllife) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (3.11.3)\n",
            "Building wheels for collected packages: dgllife\n",
            "  Building wheel for dgllife (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dgllife: filename=dgllife-0.2.6-cp37-none-any.whl size=205837 sha256=5d0ce09bf6ee13b86196d8f608def4951a2176820723410e2492f1fbdf136b95\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/29/39/54bbf37c8999f605d347805d4dec838e9ca8a894c90b119600\n",
            "Successfully built dgllife\n",
            "Installing collected packages: dgllife\n",
            "Successfully installed dgllife-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4N2Z66vjzuY"
      },
      "source": [
        "We can now import the `deepchem` package to play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmQXbIX0jzuZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae5c6efc-c342-407b-8baa-ea8b1aa686f2"
      },
      "source": [
        "import deepchem as dc\n",
        "dc.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0.dev'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gosLgS7Zjzuh"
      },
      "source": [
        "## Using GraphConv featuriser\n",
        "\n",
        "Implementing and recording the baseline for Tox21 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-pWelLHjzui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a4d225-8f52-4c3e-fd93-32bc4a7e5afb"
      },
      "source": [
        "featurizer = dc.feat.MolGraphConvFeaturizer()\n",
        "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer=featurizer)\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "print(train_dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Failed to featurize datapoint 95, [I-].[K+]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 255, [Hg+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 659, [Ba+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 985, [TlH2+]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 1423, [Cr+3]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 1534, [Fe+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 1722, [Co+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 1933, [PbH2+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 2147, [Fe+3]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 2251, [Cu+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 2760, [Cd+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 2832, [SnH2+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 4024, [Mn+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 4375, [Be+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 4611, [Zn+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 5942, [Br-].[Na+]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 6477, [Ca+2].[Cl-].[Cl-]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 6547, [SbH6+3]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n",
            "Failed to featurize datapoint 6717, [Ni+2]. Appending empty array\n",
            "Exception message: zero-size array to reduction operation maximum which has no identity\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<DiskDataset X.shape: (6249,), y.shape: (6249, 12), w.shape: (6249, 12), task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSmQ-U6Xjzup"
      },
      "source": [
        "ECFP featurizer is used. Extended Connectivity Fingerprints  is a **fingerprinting** method. They are also sometimes called \"circular fingerprints\". The ECFP algorithm begins by classifying atoms based only on their direct properties and bonds. \n",
        "\n",
        "Each unique pattern is a feature. For example, \"carbon atom bonded to two hydrogens and two heavy atoms\" would be a feature, and a particular element of the fingerprint is set to 1 for any molecule that contains that feature. It then iteratively identifies new features by looking at larger circular neighborhoods. \n",
        "\n",
        "One specific feature bonded to two other specific features becomes a higher level feature, and the corresponding element is set for any molecule that contains it. This continues for a fixed number of iterations, most often two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w0OhPYzjzu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74da8fa-9e3a-4dc0-a28e-219ab736060f"
      },
      "source": [
        "tasks"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NR-AR',\n",
              " 'NR-AR-LBD',\n",
              " 'NR-AhR',\n",
              " 'NR-Aromatase',\n",
              " 'NR-ER',\n",
              " 'NR-ER-LBD',\n",
              " 'NR-PPAR-gamma',\n",
              " 'SR-ARE',\n",
              " 'SR-ATAD5',\n",
              " 'SR-HSE',\n",
              " 'SR-MMP',\n",
              " 'SR-p53']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "denSanJbtVcS"
      },
      "source": [
        "Above are the tasks in the Tox21 dataset - there are 12 tasks, each corresponding to different biotoxicity targets, such as cell receptors and stress response pathways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfOwqhtAjzvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05161dd3-74fa-4998-9cda-6a4a044f9b35"
      },
      "source": [
        "print(datasets[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DiskDataset X.shape: (6249,), y.shape: (6249, 12), w.shape: (6249, 12), task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJrGU39bwDXY"
      },
      "source": [
        "There are three dataset objects - train split, val split and test split. Each split consists of X and y - X is the features and y is the output label. \n",
        "\n",
        "For example the train split has X.shape (6249, ) and y.shape (6249, 12). This implies that there are 6249 samples in the train split. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg-thg7gwhY4"
      },
      "source": [
        "##Training the GCNNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C0c1jW-wb9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55091c7-9c68-4005-a172-211ae24c04a5"
      },
      "source": [
        "from deepchem.models import GCNModel\n",
        "model = GCNModel(mode='classification', n_tasks=len(tasks))\n",
        "model.fit(train_dataset, nb_epoch=50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "0.4454317855834961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjOro0o0xA08"
      },
      "source": [
        "`MultitaskClassifier` is a simple stack of fully connected layers. A single hidden layer of width 1000 is used. Each input will have 1024 features, and it should produce predictions for 12 different tasks.\n",
        "\n",
        "Note that the above network is performing multitask learning - a single network is used for all 12 tasks. This is because inter task correlations exist in the data, and to take if advantage of this single neural network is used for multiple tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUaR0A5HxsQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0834bb-1b9b-43e3-f1f1-2d50e4355831"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric], transformers))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set score: {'roc_auc_score': 0.9363091418658644}\n",
            "test set score: {'roc_auc_score': 0.7183378974396456}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xesI98ChyhdV"
      },
      "source": [
        "The training set score is much higher than test set score. This indicates overfitting - and is why metrics on the validation set need to be measured in otder to tune parameters and detect overfitting."
      ]
    }
  ]
}