{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lipo_Overview.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW-q4kpKnwm6"
      },
      "source": [
        "##Installing DeepChem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glV7StXjjzt8",
        "outputId": "ab300192-1b6c-4dd2-e7e8-1033c3029d02"
      },
      "source": [
        "!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import conda_installer\n",
        "conda_installer.install()\n",
        "!/root/miniconda/bin/conda info -e"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3490  100  3490    0     0  12973      0 --:--:-- --:--:-- --:--:-- 12973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
            "python version: 3.6.9\n",
            "fetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "done\n",
            "installing miniconda to /root/miniconda\n",
            "done\n",
            "installing rdkit, openmm, pdbfixer\n",
            "added conda-forge to channels\n",
            "added omnia to channels\n",
            "done\n",
            "conda packages installation finished!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "# conda environments:\n",
            "#\n",
            "base                  *  /root/miniconda\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9dHE2sjzuG",
        "outputId": "0fe5e3eb-804c-4048-982b-501f64bc68e7"
      },
      "source": [
        "!pip install --pre deepchem"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepchem\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/8c/b9565ff7aaa043464e3c032e3b540a1524f4a89005e4d760c0feeebec4b0/deepchem-2.5.0.dev20210129140048-py3-none-any.whl (533kB)\n",
            "\r\u001b[K     |▋                               | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 21.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40kB 9.6MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 481kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 491kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 501kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 512kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 522kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 532kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
            "Installing collected packages: deepchem\n",
            "Successfully installed deepchem-2.5.0.dev20210129140048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4N2Z66vjzuY"
      },
      "source": [
        "We can now import the `deepchem` package to play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dmQXbIX0jzuZ",
        "outputId": "33de83fa-afcb-458e-dd74-05052d8dee10"
      },
      "source": [
        "import deepchem as dc\n",
        "dc.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0.dev'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gosLgS7Zjzuh"
      },
      "source": [
        "## Baseline - Fingerprints + NN\n",
        "\n",
        "Implementing and recording the baseline for Tox21 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-pWelLHjzui",
        "outputId": "870dfaf3-2ce6-442d-d647-beafa92a794e"
      },
      "source": [
        "tasks, datasets, transformers = dc.molnet.load_lipo(featurizer='ECFP')\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "print(train_dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DiskDataset X.shape: (3360, 1024), y.shape: (3360, 1), w.shape: (3360, 1), task_names: ['exp']>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSmQ-U6Xjzup"
      },
      "source": [
        "ECFP featurizer is used. Extended Connectivity Fingerprints  is a **fingerprinting** method. They are also sometimes called \"circular fingerprints\". The ECFP algorithm begins by classifying atoms based only on their direct properties and bonds. Each unique pattern is a feature. For example, \"carbon atom bonded to two hydrogens and two heavy atoms\" would be a feature, and a particular element of the fingerprint is set to 1 for any molecule that contains that feature. It then iteratively identifies new features by looking at larger circular neighborhoods. One specific feature bonded to two other specific features becomes a higher level feature, and the corresponding element is set for any molecule that contains it. This continues for a fixed number of iterations, most often two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w0OhPYzjzu7",
        "outputId": "64455e9a-c8dc-4aed-dd07-d52903251cbc"
      },
      "source": [
        "tasks"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['exp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "denSanJbtVcS"
      },
      "source": [
        "Above is the task in Lipo dataset -  we have 1 tasks, corresponding to different lipophilicity targets, such as cell receptors and stress response pathways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfOwqhtAjzvA",
        "outputId": "590887f2-9767-4bc1-f989-886d85e68298"
      },
      "source": [
        "datasets"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<DiskDataset X.shape: (3360, 1024), y.shape: (3360, 1), w.shape: (3360, 1), task_names: ['exp']>,\n",
              " <DiskDataset X.shape: (420, 1024), y.shape: (420, 1), w.shape: (420, 1), ids: ['Nc1nccc(\\\\C=C\\\\c2ccc(Cl)cc2)n1'\n",
              "  'CC(=O)Nc1ccc2ccn(c3cc(NCCCN4CCCC4)n5ncc(C#N)c5n3)c2c1'\n",
              "  'OC(CN1CCOCC1)c2ccccc2' ...\n",
              "  'COc1cc(CCc2cc(Nc3ccnc(NCc4ccccn4)n3)n[nH]2)cc(OC)c1'\n",
              "  'CCN(CC)C(=O)c1ccc(cc1)C(=C2CCN(Cc3ccc(F)cc3)CC2)c4cccnc4'\n",
              "  'CC(C)(C(=O)Nc1oc(nn1)C(=O)Nc2ccc(cc2)N3CCOCC3)c4ccc(Cl)cc4'], task_names: ['exp']>,\n",
              " <DiskDataset X.shape: (420, 1024), y.shape: (420, 1), w.shape: (420, 1), ids: ['O[C@@H](CNCCCOCCNCCc1cccc(Cl)c1)c2ccc(O)c3NC(=O)Sc23'\n",
              "  'NC1=NN(CC1)c2cccc(c2)C(F)(F)F'\n",
              "  'COc1cc(ccc1Cn2ccc3ccc(cc23)C(=O)NCC4CCCC4)C(=O)NS(=O)(=O)c5ccccc5' ...\n",
              "  'CCCSc1ncccc1C(=O)N2CCCC2c3ccncc3' 'Oc1ncnc2scc(c3ccsc3)c12'\n",
              "  'OC1(CN2CCC1CC2)C#Cc3ccc(cc3)c4ccccc4'], task_names: ['exp']>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJrGU39bwDXY"
      },
      "source": [
        "There are three dataset objects - train split, val split and test split. Each split consists of X and y - X is the features and y is the output label. Froe example the train split has X.shape (3360, 1024)\n",
        "and y.shape (3360, 1). This implies that there are 3360 samples in the train split - and each sample is represented by an ECFP vector of size 1024. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg-thg7gwhY4"
      },
      "source": [
        "##Training a Model on Fingerprints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C0c1jW-wb9R"
      },
      "source": [
        "model = dc.models.MultitaskClassifier(n_tasks=1, n_features=1024, layer_sizes=[1000])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjOro0o0xA08"
      },
      "source": [
        "`MultitaskClassifier` is a simple stack of fully connected layers. A single hidden layer of width 1000 is used. Each input will have 1024 features, and it should produce predictions for 1 task.\n",
        "\n",
        "Note that the above network is performing learning - a single network is used for the task. This is because inter task correlations exist in the data, and to take if advantage of this single neural network is used for multiple tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "WUaR0A5HxsQu",
        "outputId": "1e333381-77b0-44b7-91cc-c010843659a2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "model.fit(train_dataset, nb_epoch=10)\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
        "print('training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
        "print('test set score:', model.evaluate(test_dataset, [metric], transformers))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7f9eadcf160d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training set score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deepchem/models/keras_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             deterministic=deterministic), max_checkpoints_to_keep,\n\u001b[0;32m--> 324\u001b[0;31m         checkpoint_interval, restore, variables, loss, callbacks, all_losses)\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m   def fit_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deepchem/models/keras_model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;31m# Main training loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_training_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deepchem/models/fcnet.py\u001b[0m in \u001b[0;36mdefault_generator\u001b[0;34m(self, dataset, epochs, mode, deterministic, pad_batches)\u001b[0m\n\u001b[1;32m    169\u001b[0m           pad_batches=pad_batches):\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_b\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m           y_b = to_one_hot(y_b.flatten(), self.n_classes).reshape(\n\u001b[0m\u001b[1;32m    172\u001b[0m               -1, self.n_tasks, self.n_classes)\n\u001b[1;32m    173\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/deepchem/metrics/metric.py\u001b[0m in \u001b[0;36mto_one_hot\u001b[0;34m(y, n_classes)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y must be a vector of shape (N,) or (N, 1)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y has more than n_class unique elements.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m   \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m   \u001b[0my_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y has more than n_class unique elements."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xesI98ChyhdV"
      },
      "source": [
        "The training set score is much higher than test set score. This indicates overfitting - and is why metrics on the validation set need to be measured in otder to tune parameters and detect overfitting."
      ]
    }
  ]
}